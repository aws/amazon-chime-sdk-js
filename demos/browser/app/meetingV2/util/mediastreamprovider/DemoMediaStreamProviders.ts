// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0

import { ContentShareMediaStreamBroker, DefaultBrowserBehavior, DefaultDeviceController, Logger, VideoTransformDevice } from 'amazon-chime-sdk-js';
import MediaStreamProvider from './MediaStreamProvider';

/**
 * [[AudioBufferMediaStreamProvider]] creates a `MediaStream` from a parsed
 * audio buffer file.
 */
export class AudioBufferMediaStreamProvider implements MediaStreamProvider {
  mediaElementSource: HTMLMediaElement | undefined = undefined;

  constructor(
    private audioPath: string, private shouldLoop: boolean = false
  ) { }

  async getMediaStream(): Promise<MediaStream> {
    try {
      const resp = await fetch(this.audioPath);
      const bytes = await resp.arrayBuffer();
      const audioData = new TextDecoder('utf8').decode(bytes);
      const audio = new Audio('data:audio/mpeg;base64,' + audioData);
      audio.loop = this.shouldLoop;
      audio.crossOrigin = 'anonymous';
      audio.play();
      this.mediaElementSource = audio;

      // @ts-ignore
      const audioContext = new (window.AudioContext || window.webkitAudioContext)();
      const streamDestination = audioContext.createMediaStreamDestination();
      const mediaElementSource = audioContext.createMediaElementSource(audio);
      mediaElementSource.connect(streamDestination);
      return streamDestination.stream;
    } catch (e) {
      console.log(`Error fetching audio from ${this.audioPath}: ${e}`);
      return null;
    }
  }

  pause(): void {
    this.mediaElementSource.pause();
  }

  resume(): void {
    this.mediaElementSource.play();
  }
}

/**
* [[SynthesizedStereoMediaStreamProvider]] generates a stereo tone by using 2 `OsciallatorNode`s that
* produce 2 different frequencies. The output of these 2  nodes is passed through a `ChannelMergerNode` to obtain
* an audio stream with stereo channels where the left channel contains the samples genrated by one node and the
* right channel contains samples generated by the other.
*/
export class SynthesizedStereoMediaStreamProvider implements MediaStreamProvider {
  mediaElementSource: HTMLMediaElement | undefined = undefined;

  constructor(
    private toneHzLeft: number, private toneHzRight: number
  ) { }

  async getMediaStream(): Promise<MediaStream> {
    const audioContext = DefaultDeviceController.getAudioContext();
    const outputNode = audioContext.createMediaStreamDestination();
    const gainNode = audioContext.createGain();
    gainNode.gain.value = 0.1;
    gainNode.connect(outputNode);
    const oscillatorNodeLeft = audioContext.createOscillator();
    oscillatorNodeLeft.frequency.value = this.toneHzLeft;
    const oscillatorNodeRight = audioContext.createOscillator();
    oscillatorNodeRight.frequency.value = this.toneHzRight;
    const mergerNode = audioContext.createChannelMerger(2);
    oscillatorNodeLeft.connect(mergerNode, 0, 0);
    oscillatorNodeRight.connect(mergerNode, 0, 1);
    mergerNode.connect(gainNode);
    oscillatorNodeLeft.start();
    oscillatorNodeRight.start();
    return outputNode.stream;
  }

  pause(): void {
    // No point since synthesized
  }

  resume(): void {
    // No point since synthesized
  }
}

/**
* [[AudioGainMediaStreamProvider]] wraps another [[MediaStreamProvider]] and applies some amount of audio gain.
* It will discard any video tracks in the process, use [[MergedMediaStreamProvider]] if needed to recombine.
*/
export class AudioGainMediaStreamProvider implements MediaStreamProvider {
  private context = new AudioContext();

  constructor(private streamProvider: MediaStreamProvider, private gain: number) { }

  async getMediaStream(): Promise<MediaStream> {
    var mediaStreamSource = this.context.createMediaStreamSource(await this.streamProvider.getMediaStream());
    var destination = this.context.createMediaStreamDestination();
    var gain = this.context.createGain();
    gain.gain.value = this.gain;
    mediaStreamSource.connect(gain);
    gain.connect(destination);
    return destination.stream;
  }

  pause(): void {
    this.streamProvider.pause()
  }

  resume(): void {
    this.streamProvider.resume();
  }
}

/**
 * [[ScreenShareMediaStreamProvider]] wraps the `MediaStream` returned by a `getUserMediaCall`
 */
export class ScreenShareMediaStreamProvider implements MediaStreamProvider {
  private mediaBroker: ContentShareMediaStreamBroker;
  private mediaStream: MediaStream | undefined = undefined;

  constructor(private logger: Logger, private framerate: number) {
    this.mediaBroker = new ContentShareMediaStreamBroker(this.logger);
  }

  async getMediaStream(): Promise<MediaStream> {
    if (this.mediaStream !== undefined) {
      return Promise.resolve(this.mediaStream);
    }
    this.mediaStream = await this.mediaBroker.acquireScreenCaptureDisplayInputStream(
      undefined,
      this.framerate
    );
    return this.mediaStream;
  }

  pause(): void {
    // Nothing to pause
  }

  resume(): void {
    // Nothing to resume
  }
}

/**
 * [[FileMediaStreamProvider]] emits a media stream corresponding to audio/video found at the 
 * provided URI.
 */
export class FileMediaStreamProvider implements MediaStreamProvider {
  mediaElementSource: HTMLVideoElement | undefined = undefined;

  constructor(path: string) {
    this.mediaElementSource = document.getElementById('content-share-video') as HTMLVideoElement;
    this.mediaElementSource.src = path;
  }

  async getMediaStream(): Promise<MediaStream> {
    return this.playToStream(this.mediaElementSource);
  }

  private async playToStream(videoFile: HTMLVideoElement): Promise<MediaStream> {
    await videoFile.play();

    if (new DefaultBrowserBehavior().hasFirefoxWebRTC()) {
      // @ts-ignore
      return videoFile.mozCaptureStream();
    }

    // @ts-ignore
    return videoFile.captureStream();
  }

  pause(): void {
    this.mediaElementSource.pause();
  }

  resume(): void {
    this.mediaElementSource.play();
  }
}

/**
 * [[VideoTransformDeviceMediaStreamProvider]] emits a media stream corresponding to a [[VideoTransformDevice]]
 */
export class VideoTransformDeviceMediaStreamProvider implements MediaStreamProvider {
  constructor(private streamProvider: MediaStreamProvider, private transformDevice: VideoTransformDevice) { }

  async getMediaStream(): Promise<MediaStream> {
    return this.transformDevice.transformStream(await this.streamProvider.getMediaStream());
  }

  pause(): void {
    this.streamProvider.pause()
  }

  resume(): void {
    this.streamProvider.resume();
  }
}

/**
 * [[MergedMediaStreamProvider]] combines the audio from one [[MediaStreamProvider]] with
 * video from another [[MediaStreamProvider]]
 */
export class MergedMediaStreamProvider implements MediaStreamProvider {
  private outputStream = new MediaStream();

  constructor(private audioStream: MediaStreamProvider, private videoStream: MediaStreamProvider) { }

  async getMediaStream(): Promise<MediaStream> {
    // WARNING: For currently unknown reasons this only works when cloned audio tracks are 
    // added first. If added second no audio will be sent.

    for (const videoTrack of (await this.videoStream.getMediaStream()).getVideoTracks()) {
      console.log(`Adding video track ${videoTrack.id} to merged media stream`)
      this.outputStream.addTrack(videoTrack.clone());
    }
    for (const audioTrack of (await this.audioStream.getMediaStream()).getAudioTracks()) {
      console.log(`Adding audio track ${audioTrack.id} to merged media stream`)
      this.outputStream.addTrack(audioTrack.clone());
    }
    return Promise.resolve(this.outputStream);
  }

  pause(): void {
    this.audioStream.pause();
    this.videoStream.pause();
  }

  resume(): void {
    this.audioStream.resume();
    this.videoStream.resume();
  }
}
